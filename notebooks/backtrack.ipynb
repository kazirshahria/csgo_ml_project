{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import datetime as dt\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "project_root = os.path.abspath(os.path.join(\"..\", \"scraper\"))\n",
    "sys.path.insert(0, project_root)\n",
    "from utils import database_connection_and_cursor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def database_table(table_name: str):\n",
    "    connection, cursor = database_connection_and_cursor(\"CSGO\")\n",
    "    cursor.execute(f\"SELECT * FROM {table_name}\")\n",
    "    data_fetched = cursor.fetchall()\n",
    "    columns = [col_name[0] for col_name in cursor.description]\n",
    "    data_df = pd.DataFrame(data_fetched, columns=columns).infer_objects()\n",
    "    connection.close()\n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_projs = database_table(\"prizepicks_lines_proj\")\n",
    "pp_projs[\"game_date\"] = pd.to_datetime(pp_projs[\"game_date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def results_by_days(data: pd.DataFrame, n_days: int = 1, specific_day: bool = True):\n",
    "    # Define target date\n",
    "    target_day = (dt.datetime.today() - dt.timedelta(days=n_days)).date()\n",
    "\n",
    "    # Filter by specific day or range\n",
    "    if (n_days == 1) | (specific_day):\n",
    "        data = data[data[\"game_date\"] == str(target_day)]\n",
    "    else:\n",
    "        data = data[data[\"game_date\"] >= str(target_day)]\n",
    "\n",
    "    # Define MAP level categorization based on 'stat_type' column\n",
    "    def map_level(stat_type):\n",
    "        if \"MAPS 1-2\" in stat_type:\n",
    "            return \"MAPS 1-2\"\n",
    "        elif \"MAPS 1-3\" in stat_type:\n",
    "            return \"MAPS 1-3\"\n",
    "        elif \"MAPS 1\" in stat_type:\n",
    "            return \"MAPS 1\"\n",
    "        elif \"MAPS 3\" in stat_type:\n",
    "            return \"MAPS 3\"\n",
    "        else:\n",
    "            return \"Unknown\"\n",
    "\n",
    "    # Apply MAP level categorization\n",
    "    data[\"MAP Level\"] = data[\"stat_type\"].apply(map_level)\n",
    "\n",
    "    # Initialize results container\n",
    "    results_by_map = {}\n",
    "\n",
    "    # Loop through each MAP level and calculate statistics\n",
    "    for map_level in data[\"MAP Level\"].unique():\n",
    "        map_data = data[data[\"MAP Level\"] == map_level]\n",
    "\n",
    "        predicted_overs_and_unders = np.where(\n",
    "            map_data[\"model_projection\"] <= map_data[\"line_score\"], \"Under\", \"Over\"\n",
    "        )\n",
    "        actual_overs_and_unders = np.where(\n",
    "            map_data[\"actual_result\"] <= map_data[\"line_score\"], \"Under\", \"Over\"\n",
    "        )\n",
    "\n",
    "        # Calculate correct Over and Under predictions\n",
    "        correct_overs = np.sum((predicted_overs_and_unders == \"Over\") & (actual_overs_and_unders == \"Over\"))\n",
    "        correct_unders = np.sum((predicted_overs_and_unders == \"Under\") & (actual_overs_and_unders == \"Under\"))\n",
    "\n",
    "        # Predicted counts\n",
    "        predicted_overs = np.sum(predicted_overs_and_unders == \"Over\")\n",
    "        predicted_unders = np.sum(predicted_overs_and_unders == \"Under\")\n",
    "\n",
    "        # Percentages for Over and Under accuracy, relative to each prediction type\n",
    "        correct_over_percentage_relative = (correct_overs / predicted_overs) * 100 if predicted_overs > 0 else 0\n",
    "        correct_under_percentage_relative = (correct_unders / predicted_unders) * 100 if predicted_unders > 0 else 0\n",
    "\n",
    "        # Overall accuracy for each MAP level\n",
    "        map_results = map_data[\"hit\"].value_counts() / len(map_data)\n",
    "        overall_accuracy = round(map_results.get(\"Correct\", 0) * 100, 2)\n",
    "\n",
    "        # Store results for the current MAP level\n",
    "        results_by_map[map_level] = {\n",
    "            \"Total Predictions\": len(map_data),\n",
    "            \"Overall Accuracy (%)\": overall_accuracy,\n",
    "            \"Over Hit Rate (%)\": f\"{round(correct_over_percentage_relative, 2)}% ({correct_overs}/{predicted_overs})\",\n",
    "            \"Under Hit Rate (%)\": f\"{round(correct_under_percentage_relative, 2)}% ({correct_unders}/{predicted_unders})\",\n",
    "        }\n",
    "\n",
    "    # Calculate overall metrics across all MAP levels\n",
    "    predicted_overs_and_unders_all = np.where(\n",
    "        data[\"model_projection\"] <= data[\"line_score\"], \"Under\", \"Over\"\n",
    "    )\n",
    "    actual_overs_and_unders_all = np.where(\n",
    "        data[\"actual_result\"] <= data[\"line_score\"], \"Under\", \"Over\"\n",
    "    )\n",
    "\n",
    "    # Calculate correct Over and Under predictions for overall data\n",
    "    correct_overs_all = np.sum((predicted_overs_and_unders_all == \"Over\") & (actual_overs_and_unders_all == \"Over\"))\n",
    "    correct_unders_all = np.sum((predicted_overs_and_unders_all == \"Under\") & (actual_overs_and_unders_all == \"Under\"))\n",
    "\n",
    "    # Predicted counts for overall data\n",
    "    predicted_overs_all = np.sum(predicted_overs_and_unders_all == \"Over\")\n",
    "    predicted_unders_all = np.sum(predicted_overs_and_unders_all == \"Under\")\n",
    "\n",
    "    # Percentages for correct Over and Under predictions overall\n",
    "    correct_over_percentage_relative_all = (correct_overs_all / predicted_overs_all) * 100 if predicted_overs_all > 0 else 0\n",
    "    correct_under_percentage_relative_all = (correct_unders_all / predicted_unders_all) * 100 if predicted_unders_all > 0 else 0\n",
    "\n",
    "    # Overall accuracy for the entire dataset\n",
    "    overall_results = data[\"hit\"].value_counts() / len(data)\n",
    "    overall_accuracy_all = round(overall_results.get(\"Correct\", 0) * 100, 2)\n",
    "\n",
    "    # Store overall results\n",
    "    results_by_map[\"Overall\"] = {\n",
    "        \"Total Predictions\": len(data),\n",
    "        \"Overall Accuracy (%)\": overall_accuracy_all,\n",
    "        \"Over Hit Rate (%)\": f\"{round(correct_over_percentage_relative_all, 2)} ({correct_overs_all}/{predicted_overs_all})\",\n",
    "        \"Under Hit Rate (%)\": f\"{round(correct_under_percentage_relative_all, 2)} ({correct_unders_all}/{predicted_unders_all})\",\n",
    "    }\n",
    "\n",
    "    # Print the results\n",
    "    for level, metrics in results_by_map.items():\n",
    "        print(f\"\\nMAP Level: {level}\")\n",
    "        print(f\"Total Predictions: {metrics['Total Predictions']}\")\n",
    "        print(f\"Overall Accuracy: {metrics['Overall Accuracy (%)']}%\")\n",
    "        print(f\"Over Hit Rate: {metrics['Over Hit Rate (%)']}\")\n",
    "        print(f\"Under Hit Rate: {metrics['Under Hit Rate (%)']}\")\n",
    "\n",
    "    # Return data and results dictionary for further analysis if needed\n",
    "    data = data.reset_index(drop=True)\n",
    "    return data, results_by_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yesterday's Result\n",
      "\n",
      "MAP Level: MAPS 1-2\n",
      "Total Predictions: 48\n",
      "Overall Accuracy: 56.25%\n",
      "Over Hit Rate: 61.9% (26/42)\n",
      "Under Hit Rate: 16.67% (1/6)\n",
      "\n",
      "MAP Level: MAPS 3\n",
      "Total Predictions: 2\n",
      "Overall Accuracy: 100.0%\n",
      "Over Hit Rate: 100.0% (2/2)\n",
      "Under Hit Rate: 0% (0/0)\n",
      "\n",
      "MAP Level: Overall\n",
      "Total Predictions: 50\n",
      "Overall Accuracy: 58.0%\n",
      "Over Hit Rate: 63.64 (28/44)\n",
      "Under Hit Rate: 16.67 (1/6)\n"
     ]
    }
   ],
   "source": [
    "print(\"Yesterday's Result\")\n",
    "data, res_map = results_by_days(data=pp_projs, n_days=1, specific_day=True)\n",
    "data.to_csv(\"yesterdays_hit_rate.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 Days Result\n",
      "\n",
      "MAP Level: MAPS 1-2\n",
      "Total Predictions: 158\n",
      "Overall Accuracy: 60.76%\n",
      "Over Hit Rate: 62.41% (83/133)\n",
      "Under Hit Rate: 40.0% (10/25)\n",
      "\n",
      "MAP Level: MAPS 3\n",
      "Total Predictions: 28\n",
      "Overall Accuracy: 64.29%\n",
      "Over Hit Rate: 53.57% (15/28)\n",
      "Under Hit Rate: 0% (0/0)\n",
      "\n",
      "MAP Level: Overall\n",
      "Total Predictions: 186\n",
      "Overall Accuracy: 61.29%\n",
      "Over Hit Rate: 60.87 (98/161)\n",
      "Under Hit Rate: 40.0 (10/25)\n"
     ]
    }
   ],
   "source": [
    "n_days = int(input(\"Enter the number of days you'd like to look back: \"))\n",
    "print(f\"{n_days} Days Result\")\n",
    "data, res_map = results_by_days(data=pp_projs, n_days=n_days, specific_day=False)\n",
    "data.to_csv(f\"{n_days}day_hit_rate.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csgo_ml_project-AzE7bgf4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
